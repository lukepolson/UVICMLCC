{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from numpy.random import exponential\n",
    "import random\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 8\n",
    "MAX_HEALTH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code for Snake Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake():\n",
    "    \n",
    "    # Initialize\n",
    "    def __init__(self, BOARD_SIZE, MAX_HEALTH):  \n",
    "        # Set Initial Position\n",
    "        self.path_X = deque()\n",
    "        self.path_X.append(np.random.randint(low=2, high=BOARD_SIZE-2))\n",
    "        self.path_X.append(self.path_X[0])\n",
    "        self.path_X.append(self.path_X[0])\n",
    "        self.path_Y = deque()\n",
    "        self.path_Y.append(np.random.randint(low=2, high=BOARD_SIZE-2))\n",
    "        self.path_Y.append(self.path_Y[0]+1)\n",
    "        self.path_Y.append(self.path_Y[0]+2)\n",
    "        \n",
    "        # Set Initial Health\n",
    "        self.max_health = MAX_HEALTH\n",
    "        self.health = MAX_HEALTH\n",
    "        \n",
    "        # Set if snake has just eaten\n",
    "        self.just_eaten = False\n",
    "    \n",
    "    # Return all coordinates\n",
    "    def get_path_coords(self):\n",
    "        return self.path_X, self.path_Y\n",
    "    \n",
    "    def get_head_coords(self):\n",
    "        return self.path_X[0], self.path_Y[0]\n",
    "    \n",
    "    def get_neck_coords(self):\n",
    "        return self.path_X[1], self.path_Y[1]\n",
    "    \n",
    "    def get_tail_coords(self):\n",
    "        return self.path_X[-1], self.path_Y[-1]\n",
    "    \n",
    "    # Return health\n",
    "    def reduce_health(self):\n",
    "        self.health-=1\n",
    "    \n",
    "    def get_health(self):\n",
    "        return self.health\n",
    "    \n",
    "    def set_full_health(self):\n",
    "        self.health = self.max_health\n",
    "        \n",
    "    def get_just_eaten(self):\n",
    "        return self.just_eaten\n",
    "    \n",
    "    def set_just_eaten(self, boolean):\n",
    "        self.just_eaten = boolean\n",
    "    \n",
    "    def move(self, action):\n",
    "        i=0; j=0\n",
    "        if (action==0):\n",
    "            i = 1\n",
    "        elif (action==1):\n",
    "            j = 1\n",
    "        elif (action==2):\n",
    "            i = -1\n",
    "        elif (action==3):\n",
    "            j = -1\n",
    "        else:\n",
    "            raise ValueError('`action` should be 0 or 1 or 2 or 3.')\n",
    "        self.path_X.appendleft(self.path_X[0]+i)\n",
    "        self.path_Y.appendleft(self.path_Y[0]+j)\n",
    "        if not self.just_eaten:\n",
    "            self.path_X.pop()\n",
    "            self.path_Y.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeEnv(py_environment.PyEnvironment):\n",
    "    \n",
    "    def reset_board(self):\n",
    "        \n",
    "        # Create Snakes\n",
    "        self.master_snake = Snake(BOARD_SIZE, MAX_HEALTH)\n",
    "        \n",
    "        # Create Board\n",
    "        self._state = [([0]*BOARD_SIZE) for i in range(BOARD_SIZE)]\n",
    "        self._episode_ended = False \n",
    "        master_x_coords, master_y_coords = self.master_snake.get_path_coords()\n",
    "        for i, (x, y) in enumerate(zip(master_x_coords, master_y_coords)):\n",
    "                if i == 0:\n",
    "                    self._state[y][x] = -1\n",
    "                else:\n",
    "                    self._state[y][x] = 1\n",
    "    \n",
    "        # Set food timer\n",
    "        self.food_spawn_arr = np.ceil(exponential(5, size=100))\n",
    "        self.current_food_spawn = 0 #current index of above array\n",
    "        self.food_timer = 0\n",
    "        # Maybe store other snakes healths in an array?\n",
    "\n",
    "        # Set if snake ate during previous turn\n",
    "        self.just_eaten = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(BOARD_SIZE,BOARD_SIZE), dtype=np.int32, minimum=-1, name='observation')\n",
    "        self.reset_board()\n",
    "        \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.reset_board()\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "            \n",
    "        \n",
    "    def foodspawn_assist(self):\n",
    "        coords = list(zip(np.where(np.array(self._state)==0)[0],\n",
    "                          np.where(np.array(self._state)==0)[1]))\n",
    "        coord = random.choice(coords)\n",
    "        self._state[coord[0]][coord[1]] = 2\n",
    "        return None\n",
    "\n",
    "    def _step(self, action):\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        # Reduce Health\n",
    "        self.master_snake.reduce_health()\n",
    "        \n",
    "        # Get Previous tail coordinates\n",
    "        master_tail_x, master_tail_y = self.master_snake.get_tail_coords()\n",
    "        \n",
    "        # Apply Action\n",
    "        try: self.master_snake.move(action)\n",
    "        except ValueError as error:\n",
    "            raise ValueError('`action` should be 0 or 1 or 2 or 3.')\n",
    "        \n",
    "        # Get  New Head Coords\n",
    "        master_head_x, master_head_y = self.master_snake.get_head_coords()\n",
    "        master_neck_x, master_neck_y = self.master_snake.get_neck_coords()\n",
    "        \n",
    "        # Spawn Food\n",
    "        self.food_timer +=1\n",
    "        if (self.food_spawn_arr[self.current_food_spawn]==self.food_timer):\n",
    "            # Spawn Food\n",
    "            self.foodspawn_assist()\n",
    "            # Reset Timer\n",
    "            self.food_timer = 0\n",
    "            self.current_food_spawn = (self.current_food_spawn+1)%len(self.food_spawn_arr)\n",
    "        \n",
    "        # Check to see if out of bounds or hit itself\n",
    "        if (master_head_x == BOARD_SIZE or master_head_y == BOARD_SIZE \\\n",
    "            or master_head_x == -1 or master_head_y == -1 \\\n",
    "            or self._state[master_head_y][master_head_x] == 1):\n",
    "            self._episode_ended = True\n",
    "        \n",
    "        # Else perform snake step\n",
    "        else:\n",
    "            # Check What block snake has landed on. If 2 then\n",
    "            # it landed on food\n",
    "            block = self._state[master_head_y][master_head_x]\n",
    "\n",
    "            # Set Head Value to -1\n",
    "            self._state[master_head_y][master_head_x] = -1\n",
    "            # Set Neck to 1\n",
    "            self._state[master_neck_y][master_neck_x] = 1\n",
    "            # If snake has not just eaten then set tail location to zero\n",
    "            if not(self.master_snake.get_just_eaten()):\n",
    "                self._state[master_tail_y][master_tail_x] = 0\n",
    "            self.master_snake.set_just_eaten(False)\n",
    "\n",
    "            # Check if snake consumed food\n",
    "            if(block==2):\n",
    "                self.master_snake.set_full_health()\n",
    "                self.master_snake.set_just_eaten(True)\n",
    "\n",
    "        # If out of health then die\n",
    "        if self.master_snake.get_health() == 0:\n",
    "            return self.reset()\n",
    "        \n",
    "        # If Episode Ended\n",
    "        if self._episode_ended:\n",
    "            reward = 0\n",
    "            return ts.termination(np.array(self._state, dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "            np.array(self._state, dtype=np.int32), reward=1.0, discount=1.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = SnakeEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(0), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(2), reward=array(0., dtype=float32), discount=array(0., dtype=float32), observation=array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  2,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1,  1,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  2,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Later Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details see 649-650. For now we need to create\n",
    "1. A deep Q-Network\n",
    "2. DQN Agent (takes care of creating the collect policy from replay buffer)\n",
    "3. Replay buffer (and observer to write to it)\n",
    "4. Some training metrics\n",
    "5. The driver\n",
    "6. The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-Q Network and DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres the network. It has a preprocessing layer that normalizes all the entries. Next we apply a convolutional neural netork (chap 14). Lastly, it applies a dense layer with 512 inputs, and then automatically a dense layer with 4 outputs (one for each Q value). All these layers use ReLU activation functions (except the last one which has no activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 2.)\n",
    "conv_layer_params=[(8, 3, 2), (6, 3, 2)]\n",
    "fc_layer_params=[512] # number of neurons for each dense layer (only 1 layer here)\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(8, 8), dtype=tf.int32, name='observation', minimum=array(-1), maximum=array(2147483647))"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_env.observation_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer EncodingNetwork/conv2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [0, 8, 8]\n  In call to configurable 'DqnAgent' (<function DqnAgent.__init__ at 0x000001F3E31990D8>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-374-f9d9e2e042ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                  \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# discount factor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                  \u001b[0mtrain_step_counter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                  epsilon_greedy=lambda: epsilon_fn(train_step))\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[0mscope_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" in scope '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m         \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, time_step_spec, action_spec, q_network, optimizer, observation_and_action_constraint_splitter, epsilon_greedy, n_step_update, boltzmann_temperature, emit_log_probability, target_q_network, target_update_tau, target_update_period, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, name)\u001b[0m\n\u001b[0;32m    217\u001b[0m         observation_and_action_constraint_splitter)\n\u001b[0;32m    218\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_q_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget_q_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m       \u001b[0mtarget_q_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tf_agents\\networks\\network.py\u001b[0m in \u001b[0;36mcreate_variables\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m       self.__call__(\n\u001b[0;32m    134\u001b[0m           \u001b[0mrandom_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m           **kwargs)\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tf_agents\\networks\\network.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tf_agents\\networks\\q_network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, observation, step_type, network_state)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m    144\u001b[0m     state, network_state = self._encoder(\n\u001b[1;32m--> 145\u001b[1;33m         observation, step_type=step_type, network_state=network_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_q_value_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tf_agents\\networks\\network.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tf_agents\\networks\\encoding_network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_postprocessing_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m       \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_squash\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;31m# Eager execution on data tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2096\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[1;32m-> 2098\u001b[1;33m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[0;32m   2099\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    175\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                          str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer EncodingNetwork/conv2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [0, 8, 8]\n  In call to configurable 'DqnAgent' (<function DqnAgent.__init__ at 0x000001F3E31990D8>)"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "\n",
    "# Create variable that counts the number of training steps\n",
    "train_step = tf.Variable(0)\n",
    "# Create optimizer \n",
    "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "# Computes epsilon for epsilon greedy policy given the training step\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=10000, \n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer and Observer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-Agents provides some replay buffer implementation in the tf_agents.replay_buffers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    # Determines the data spec type\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    # The number of trajectories added at each step\n",
    "    batch_size=tf_env.batch_size,\n",
    "    # This can store 1 million trajectories (note: requires a lot of RAM)\n",
    "    max_length=1000000)\n",
    "\n",
    "# Create the observer that adds trajectories to the replay buffer\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class increments every time it is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These training metrics can be stored during training (useful for outputting verbose things during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics can be logged as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 0\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver explores the environment using a given policy, collects experiences, and broadcasts them to some observers (and then the observers typically place them in the replay buffer).\n",
    "1. Driver passes current **time step** to collect policy which then chooses an action and returns an **action step** object containing the action\n",
    "2. The driver passes the action to the environment, which then returns the next step\n",
    "3. The driver creates a trajectory object to represent this transition and broadcasts it to the observer\n",
    "\n",
    "There are two main types of drivers:\n",
    "1. DynamicStepDriver: Collects experiences for certain number of steps\n",
    "2. DynamicEpisodeDriver: Collects experiences for certain number of episodes\n",
    "\n",
    "In our case we want to for four steps for each training iteration (4 steps allows us to see where ball is going). We create it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env, # Env to play with\n",
    "    agent.collect_policy, # Collect policy of the agent\n",
    "    observers=[replay_buffer_observer] + train_metrics, # pass to all observers\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Replay Buffer with Experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps=20000) # <=> 80,000 ALE frames\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has to do with sampling batches of trajectories from the replay buffer. To sample them, you just need to call the \"get_next()\" method. This also returns a BufferInfo object that contains the sample identifiers and their sampling probabilities.\n",
    "* The following code samples a small batch of two trajectories (subepisodes) each containing three consecutive steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(888) # chosen to show an example of trajectory at the end of an episode\n",
    "\n",
    "trajectories, buffer_info = replay_buffer.get_next(\n",
    "    sample_batch_size=2, num_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can get all attributes of these trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 84, 84, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories.observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two trajectories, each with three steps, each steps observation is 84x84 x4 (4 overlaid image). Can also convert  to time_steps, action_steps, and next_time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2, 84, 84, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "time_steps.observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets convert the dataset to a tensorflow dataset so we can take advantage of parallelization. We sample batches of 64 trajectories, each trajectory with 2 steps. The data set processes three elements in parallel, and prefetches 3 batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up training we convert main functions to TensorFlow Fucntions. For this we use a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.utils.common import function\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres a function that runs training for $n$ iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    # Get initial policy state\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    # Create iterator over dataset and loop\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        # Pass current time step and policy state to get next time step and policy state\n",
    "        # Collects experience for 4 steps then broadcasts trajectories to replay buffer\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        # Sample a batch of trajectories from the dataset, pass to the train method\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 232\n",
      "\t\t EnvironmentSteps = 40004\n",
      "\t\t AverageReturn = 0.8999999761581421\n",
      "\t\t AverageEpisodeLength = 170.39999389648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997 loss:0.00757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 254\n",
      "\t\t EnvironmentSteps = 44004\n",
      "\t\t AverageReturn = 1.399999976158142\n",
      "\t\t AverageEpisodeLength = 182.39999389648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997 loss:0.00033"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 279\n",
      "\t\t EnvironmentSteps = 48004\n",
      "\t\t AverageReturn = 0.30000001192092896\n",
      "\t\t AverageEpisodeLength = 150.3000030517578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2997 loss:0.00760"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 302\n",
      "\t\t EnvironmentSteps = 52004\n",
      "\t\t AverageReturn = 0.699999988079071\n",
      "\t\t AverageEpisodeLength = 163.89999389648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999 loss:0.01490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 325\n",
      "\t\t EnvironmentSteps = 56004\n",
      "\t\t AverageReturn = 0.8999999761581421\n",
      "\t\t AverageEpisodeLength = 170.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998 loss:0.00002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 346\n",
      "\t\t EnvironmentSteps = 60004\n",
      "\t\t AverageReturn = 1.2999999523162842\n",
      "\t\t AverageEpisodeLength = 187.1999969482422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5999 loss:0.00001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 368\n",
      "\t\t EnvironmentSteps = 64004\n",
      "\t\t AverageReturn = 1.0\n",
      "\t\t AverageEpisodeLength = 174.8000030517578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6998 loss:0.00008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 388\n",
      "\t\t EnvironmentSteps = 68004\n",
      "\t\t AverageReturn = 1.399999976158142\n",
      "\t\t AverageEpisodeLength = 197.89999389648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 loss:0.00015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 410\n",
      "\t\t EnvironmentSteps = 72004\n",
      "\t\t AverageReturn = 0.8999999761581421\n",
      "\t\t AverageEpisodeLength = 176.60000610351562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8999 loss:0.00011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 432\n",
      "\t\t EnvironmentSteps = 76004\n",
      "\t\t AverageReturn = 1.399999976158142\n",
      "\t\t AverageEpisodeLength = 187.10000610351562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 loss:0.00030"
     ]
    }
   ],
   "source": [
    "train_agent(n_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
