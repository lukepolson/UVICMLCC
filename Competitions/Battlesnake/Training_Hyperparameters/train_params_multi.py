## DO NOT DELETE ANYTHING FROM THIS FILE: ONLY ADD MORE TRIALS TO DATAFRAME
import pandas as pd


# Bigger board, more food spawning
trial_0 = {
"board_size": 10,
"food_reward": 0,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"win_reward": 100,
"kill_step_reward": 0,
"food_spawn_mode": 0,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 10000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 1000000
}

trial_1 = {
"board_size": 10,
"food_reward": 10,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"win_reward": 100,
"kill_step_reward": 0,
"food_spawn_mode": 0,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 20000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 2000000
}

# No enemy policy
trial_2 = {
"board_size": 10,
"food_reward": 10,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"win_reward": 100,
"kill_step_reward": 0,
"food_spawn_mode": 0,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 1000000
}

# No enemy policy
trial_3 = {
"board_size": 10,
"food_reward": 0,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.95,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 5000000
}

# No enemy policy
trial_4 = {
"board_size": 10,
"food_reward": 0,
"max_counter": 2,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": 0,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.95,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 5000000
}

# No enemy policy
trial_5 = {
"board_size": 10,
"food_reward": 0,
"max_counter": 2,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[140, 140, 140]],
"optimizer_learning_rate": 2.5e-3,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Add CNN Network
trial_6 = {
"board_size": 10,
"food_reward": 0,
"max_counter": 2,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": [[(16, (4, 4), 1), (32, (3,3), 1)]],
"fc_layer_params": [[250]],
"optimizer_learning_rate": 2.5e-3,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_7 = {
"board_size": 10,
"food_reward": 0,
"max_counter": 2,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -10,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": [[(16, (5, 5), 1), (32, (2,2), 1), (32, (2,2), 1)]],
"fc_layer_params": [[512]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.992,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Bug fixed from trial 7
trial_8 = {
"board_size": 10,
"food_reward": 0,
"max_counter": 2,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -10,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[120, 120, 120]],
"optimizer_learning_rate": 5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.992,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# tRY AN 8X8 BOARD
trial_9 = {
"board_size": 8,
"food_reward": 0,
"max_counter": 2,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -10,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[120, 120, 120]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.992,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

trials = [trial_0, trial_1, trial_2, trial_3, trial_4, trial_5, trial_6, trial_7, trial_8, trial_9]
dfs = [pd.DataFrame(trial) for trial in trials]
df = pd.concat(dfs).reset_index()
df.to_pickle('train_params_multi.pkl')