'''
trial_0 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (4, 4), 2), (32, (3 ,3), 1)]],
"fc_layer_params": [[50, 50]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.97,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Same as trial 0, except food reward decreased |DONE|
trial_1 = {
"food_reward": 3,
"step_reward": 1,
"conv_layer_params": [[(16, (4, 4), 2), (32, (3 ,3), 1)]],
"fc_layer_params": [[50, 50]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.97,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Same as trial 0, except more NN neurons |DONE|
trial_2 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (4, 4), 2), (32, (3 ,3), 1)]],
"fc_layer_params": [[75, 75]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.97,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Same as trial 0, except diff conv_layer_params |DONE|
trial_3 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[50, 50]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.97,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Same as trial_0 except discount factor reduced |DONE|
trial_4 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (4, 4), 2), (32, (3 ,3), 1)]],
"fc_layer_params": [[50, 50]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.90,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Same as trial_0, more NN params than trial_2 |DONE|
trial_5 =  {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (4, 4), 2), (32, (3 ,3), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.97,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Same as trial_3 except more conv windows used |DONE|
trial_6 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(32, (3, 3), 1), (64, (2 ,2), 1)]],
"fc_layer_params": [[50, 50]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.97,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Uses best things found so far: smaller conv windows, more NN params, smaller discount factor |DONE|
trial_7 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.90,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# Same as trial_7 but three layers of neurons |DONE|
trial_8 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.90,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 4000000
}

# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# --------------- These trials are now testing the snake that has health awareness ------
# --------------- The snake no longer no longer has any reward for eating food and its max
# --------------- health has been increased to 50.
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------

# |DONE|
trial_9 = {
"food_reward": 0,
"step_reward": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.90,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 1000000
}

#-----------------------------------
# ----All trials below use 100 health snake and -25 reward for death  ---
#-----------------------------------


# Base trial for new snake with 100 health |DONE|

trial_10 = {
"food_reward": 0,
"step_reward": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.90,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 2000000
}

# Try rewarding for food
trial_11 = {
"food_reward": 5,
"step_reward": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.90,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 250000
}

# NEW PARAM ADDED: DEATH_REWARD- start at -100. Also decrease discount factor
trial_12 = {
"food_reward": 0,
"step_reward": 1,
"death_reward": -100,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 250000
}


# NEW PARAM ADDED: FOOD_SPAWN_MODE: Food only spawns when snake has just eaten
# NEW PARAM ADDED: FOOD_REWARD_MODE: Rewards for eating food, higher rewards when health is low. 0 is regular, 1 is new mode.

# Food reward mode 0
trial_13 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 250000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 2500000
}

## ADDED KILL STEP REWARD
# Food reward mode 1
trial_14 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Changed food reward policy
trial_15 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": [[(16, (3, 3), 1), (32, (2 ,2), 1)]],
"fc_layer_params": [[100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Skip the CNN all together and use three fc_layers. Otherwise same as trial 15
# NEW BASE
trial_16 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Try 4 hidden layers
trial_17 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Try 3 hidden layers with more neurons each
trial_18 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[150, 150, 150]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Increase discount factor
trial_19 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.9,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Reduce discount factor
trial_20 = {
"food_reward": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.8,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Increase food reward by a lot
trial_21 = {
"food_reward": 50,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.8,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 2000000
}

# Increase food reward by a lot
trial_22 = {
"food_reward": 500,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.95,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 500000
}

# Increase discount factor
trial_23 = {
"food_reward": 500,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as before now with more neurons per layer
trial_24 = {
"food_reward": 500,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[150, 150, 150]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# NEW PARAM: Health cut off- point at which one is rewarded for eating. In past trials was 50: now set to 20
trial_25 = {
"food_reward": 500,
"health_cutoff": 20,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Test with symmetry computer, no food reward
trial_26 = {
"food_reward": 0,
"health_cutoff": 20,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Try training for many iterations
trial_27 = {
"food_reward": 500,
"health_cutoff": 20,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 50000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# No more food reward since didn't seem to make a difference.
# Increase epsilon decay steps (this is the same as 26 but trains for longer
# and has longer initial random period (more epislon decay steps))

# The new base line 

trial_28 = {
"food_reward": 0,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# Larger Discount factor
trial_29 = {
"food_reward": 0,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.992,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# Larger iniial replay buffer
trial_30 = {
"food_reward": 0,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# Larger kill step reward
trial_31 = {
"food_reward": 0,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -50,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.992,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# Smaller final epsilon
trial_32 = {
"food_reward": 0,
"health_cutoff": 0,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# No enemy policy
trial_33 = {
"board_size": 10,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 20,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.95,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 5000000
}

# Tail awareness, random initial growth, lots of training
trial_34 = {
"board_size": 10,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 20,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.95,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Same as above but with food reward NOTE COUNTER MODE WAS BUGGED HERE
trial_35 = {
"board_size": 10,
"food_reward": 5,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 20,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Repeat of 35 no bug
trial_36 = {
"board_size": 10,
"food_reward": 5,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 20,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"food_reward_mode": 0,
"conv_layer_params": None,
"fc_layer_params": [[125, 125, 125]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 100000,
"epsilon_final": 0.001,
"target_update_period": 2000,
"discount_factor": 0.85,
"init_replay_buffer": 20000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Same as trial 30 but bigger board size
trial_37 = {
"board_size": 10,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Same as above but board size 8
trial_38 = {
"board_size": 8,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Same as 38 but removed 5th dimension in Snake_v3
trial_39 = {
"board_size": 8,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Removed tail stuff same as 38 otherwise
trial_40 = {
"board_size": 8,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# New simplified snake environment (Snake_v4)
trial_41 = {
"board_size": 8,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# No head block: only head block location
trial_42 = {
"board_size": 8,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Try 3 different channels: 1 for head, 1 for food, 1 for body (Snake_v5)
trial_43 = {
"board_size": 8,
"food_reward": 0,
"health_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"food_reward_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Same as above but removed health_cutoff, death/win reward, food_reward mode
# Now introducing food_reward_cutoff: after this point nothing is rewarded for eating
trial_44 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 300000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 15000000
}

# Some more testing of the food cutoff (cutoff 0)
trial_45 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 0,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 1500000
}

# Some more testing of the food cutoff (cutoff 100000)
trial_46 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 100000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 1500000
}

# Some more testing of the food cutoff (cutoff 1000000)
trial_47 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 1000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 1500000
}

# Same as above significantly increase neurons
trial_48 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 1500000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[300, 300, 300, 300]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 50000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 3000000
}

# Same as above significantly increase neurons
trial_49 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 1500000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[500, 500, 500, 500, 500]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 50000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 3000000
}

# Same as above but with health cutoff food reward back
trial_50 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 1500000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"health_cutoff": 20,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[500, 500, 500, 500, 500]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 50000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 3000000
}

trial_51 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"win_reward": 0,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 35000000
}

# Snake now has head awareness, blocks still size 2, death reward back
trial_52 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# Same as above but decrease learning rate
trial_53 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 5e-5,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# See if food reward cutoff actually works (check difference to 52)
trial_54 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 25000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# leave final epsilon larger (otherwise same as 52)
trial_55 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.1,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# leave final epsilon much larger (otherwise same as 52)
trial_56 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.5,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 2000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# Same as 52 but larger initial replay buffer
trial_57 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

trial_57 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# Try with less neurons
trial_58 = {
"board_size": 8,
"food_reward": 0,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[50, 50]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 1000000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# Deep network with few neurons per layer
trial_59 = {
"board_size": 8,
"food_reward": 0,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[50, 50, 50, 50]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 1000000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 20000000
}

# Same as 52 but 20% dropout
trial_60 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": [[0.2, 0.2, 0.2]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# Same as 52 but 10% dropout
trial_61 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": [[0.1, 0.1, 0.1]],
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 2000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# Same as 52 but larger target update period
trial_62 = {
"board_size": 8,
"food_reward": 5,
"food_reward_cutoff": 5000000,
"counter_mode": 1,
"max_counter": 3,
"step_reward": 1,
"death_reward": -100,
"kill_step_reward": -5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

'''

# A NEW ERA: NOW USING RESULTS FROM PAPER
trial_0 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"counter_mode": 1,
"max_counter": 3,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}
# Try different version of DRF (introduce reward type parameter)
trial_1 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# DRF v2 (linear multiplicative term)
trial_2 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 2,
"counter_mode": 1,
"max_counter": 3,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# DRF v3 (step function (-1 or 1) multiplicative term)
trial_3 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"counter_mode": 1,
"max_counter": 3,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# DRF_v4 (stronger positive and negative reward for snake with high health)
trial_4 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 4,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# DRF_v3 but now don't reward for eating unless snake is below certain amount of health
trial_5 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# DRF_v2 but now food_Reward_type_v2 so doesnt get big reward from eating at high health
trial_6 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 2,
"food_reward_type": 2,
"counter_mode": 1,
"max_counter": 3,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# At this point triaal 5 seemed to work the best so now we now
# do four more trials which are variants of trial 5- 1. make food cutoff lower
# 2. make food cutoff higher 3. Larger DRF for health<cutoff 4. Larger DRF health>cutoff
# Thus we introduce 3 new parameters: health_cutoff, DRF_MC_high, DRF_MC_low

trial_7 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 1,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_8 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 40,
"DRF_MC_high": 1,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}


# NOTE: BUG IN CODE MC LOW AND MC_HIGH WERE SWAPPED (Trial 9 has MC_HIGH as 1 and MC_LOW as 5)
trial_9 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 30,
"DRF_MC_high": 5,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_10 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 30,
"DRF_MC_high": 1,
"DRF_MC_low": 5,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# 7 performed the best. Now we experiment with modifications of DRF_MC_high

trial_11 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 0.5,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_12 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 0.2,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_13 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 0,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# NOTE: Previous 3 trials were bugged: now use FOOD_REWARD_TYPE_V3 which gives
# Reward of -1 if eating food before health cutoff

trial_14 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 3,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 0.5,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_15 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 3,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 0.2,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_16 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 3,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 0,
"DRF_MC_low": 1,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

## NOW TRAIN A SNAKE THAT IS VERY HUNGRY ALL THE TIME AND NAKE THAT ALWAYS DODGES FOOD
## AND COMBINE THEM LATER ON


## SNAKES THAT DONT WANT TO EAT
# No reward for moving away from food 
trial_17 = {
"board_size": 8,
"food_reward": -1,
"death_reward": -1,
"step_reward": 0.5,
"reward_type": 3,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 0,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": True,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Slight reward for moving away from food (DRF v3)
trial_18 = {
"board_size": 8,
"food_reward": -1,
"death_reward": -1,
"step_reward": 0.5,
"reward_type": 3,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 0,
"DRF_MC_high": 1,
"DRF_MC_low": 0,
"immortal": True,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

## SNAKES THAT WANT TO EAT

# Reward for moving towards food
trial_19 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 1,
"immortal": False,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# No reward for moving towards food
trial_20 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Try to give a wall penalty for the immoral snake. 21 and 22 same as 17 but with wall penalty
trial_21 = {
"board_size": 8,
"food_reward": -1,
"death_reward": -1,
"step_reward": 0.5,
"reward_type": 3,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 0,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": True,
"wall_penalty": -0.25,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

trial_22 = {
"board_size": 8,
"food_reward": -1,
"death_reward": -1,
"step_reward": 0.5,
"reward_type": 3,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 0,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": True,
"wall_penalty": -0.25,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as 7 but with wall penalty
trial_23 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 1,
"DRF_MC_low": 1,
"immortal": False,
"wall_penalty": -0.25,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# 23 worked exceptionally well so now we train for longer
trial_24 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 1,
"DRF_MC_low": 1,
"immortal": False,
"wall_penalty": -0.25,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 25000000
}

# To be used with C51 network. Same as 23
trial_25 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": 3,
"food_reward_type": 1,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 1,
"DRF_MC_low": 1,
"immortal": False,
"wall_penalty": -0.25,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"num_atoms": 51,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Reward -1 for eating food when above health cutoff and +1 when below. No distance reward functions.
trial_26 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 3,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 20,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"wall_penalty": -0.25,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 51,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}


# HUNGRY SNAKE THAT WANTS TO EAT: Simple (+1 for eating, -1 for death, no movement rewards)
# Reward for moving towards food
trial_27 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"wall_penalty": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"num_atoms": 51,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as 27 but with complex network
trial_28 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"wall_penalty": 0,
"kill_step_reward": 0,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"num_atoms": 51,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Redo of trial 27 with recommended adjustments from forum:
# n_atoms = 51, min_q_value=-1, use epsilon greedy policy for initial exploration.
# Also give -0.5 for kill step reward to stop snake from using random actions (rely on eps greedy to take these)
# No rewards for moving close to/ far away from food. THIS IS HUNGRY SNAKE
trial_29 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"num_atoms": 51,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as 29 but num_atoms=21
trial_30 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as 29 but complex network
trial_31 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 51,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Should be used to test the frame stack with snake v7 and train_c51_v2
trial_32 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": None,
"fc_layer_params": [[100, 100, 100]],
"dropout_layer_params": None,
"num_atoms": 51,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 5000000
}

# Same as 32 but complex network
trial_33 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 51,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 5000000
}

# Mixture of 30 and 31; less n_atoms and complex network
trial_34 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as 34 but n_atoms=11
trial_35 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 11,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as 34 but n_step_update=1
trial_36 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 1,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 5000000
}

# Same as 34 but n_step_update=5
trial_37 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 5,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 5000000
}

# Exact same as 34: TEMPORARY SWITCH TO SNAKE V8 WITH TAIL
trial_38 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 2), (64, 3, 1)]],
"fc_layer_params": [[512, 512]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000
}

# Same as 37 less fully connected layer neurons, slight modification to conv layer
trial_39 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 1), (64, 3, 1)]],
"fc_layer_params": [[150, 150]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 5,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000,
"n_replay_buffer": 10000000
}

# Same as 39 but smaller replay buffer
trial_40 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(64, 4, 1), (64, 3, 1)]],
"fc_layer_params": [[150, 150]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 5,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":3,
"n_iterations": 10000000,
"n_replay_buffer": 1000000
}

'''
trials = [trial_0, trial_1, trial_2, trial_3, trial_4, trial_5, trial_6, trial_7, trial_8, trial_9,
         trial_10, trial_11, trial_12, trial_13, trial_14, trial_15, trial_16, trial_17, trial_18,
         trial_19, trial_20, trial_21, trial_22, trial_23, trial_24, trial_25, trial_26, trial_27, trial_28,
         trial_29, trial_30, trial_31, trial_32, trial_33, trial_34, trial_35, trial_36, trial_37, trial_38, trial_39,
         trial_40, trial_41, trial_42, trial_43, trial_44, trial_45, trial_46, trial_47, trial_48, trial_49, trial_50,
         trial_51, trial_52, trial_53, trial_54, trial_55, trial_56, trial_57, trial_58, trial_59, trial_60, trial_61,
         trial_62, trial_63, trial_64, trial_65, trial_66]
'''