## DO NOT DELETE ANYTHING FROM THIS FILE: ONLY ADD MORE TRIALS TO DATAFRAME
import pandas as pd

# Used with regular Q-Learning (Train_v6.py - C51 params here dont do anything)
trial_0 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(32, 8, 4), (64, 4, 2)]],
"fc_layer_params": [[256]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":5,
"n_iterations": 10000000,
"n_replay_buffer": 1000000,
"frame_stack": None
}

# Used with C51 (same as above)
trial_1 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(32, 8, 4), (64, 4, 2)]],
"fc_layer_params": [[256]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":5,
"n_iterations": 10000000,
"n_replay_buffer": 1000000,
"frame_stack": None
}

# Try Q-Network With Frame Stacking (Train_v6)
trial_2 = {
"board_size": 8,
"food_reward": 1,
"death_reward": -1,
"step_reward": 0,
"reward_type": -1,
"food_reward_type": 0,
"counter_mode": 1,
"max_counter": 3,
"health_cutoff": 100,
"DRF_MC_high": 0,
"DRF_MC_low": 0,
"immortal": False,
"snake_protect": True,
"wall_penalty": 0,
"kill_step_reward": -0.5,
"food_spawn_mode": 1,
"conv_layer_params": [[(32, 8, 4), (64, 4, 2)]],
"fc_layer_params": [[256]],
"dropout_layer_params": None,
"num_atoms": 21,
"min_q_value": -1,
"max_q_value": 20,
"n_step_update": 2,
"optimizer_learning_rate": 2.5e-4,
"optimizer_decay": 0.95,
"optimizer_momentum": 0,
"optimizer_epsilon": 0.00001,
"epsilon_decay_steps": 500000,
"epsilon_final": 0.01,
"target_update_period": 20000,
"discount_factor": 0.99,
"init_replay_buffer": 200000,
"dataset_sample_batch_size": 64,
"dataset_num_steps": 2,
"dataset_num_parallel_calls":5,
"n_iterations": 10000000,
"n_replay_buffer": 1000000,
"frame_stack": 3
}


trials = [trial_0, trial_1, trial_2]


dfs = [pd.DataFrame(trial) for trial in trials]
df = pd.concat(dfs).reset_index()
df.to_pickle('train_params.pkl')